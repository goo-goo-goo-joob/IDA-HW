{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0-rc0\n",
      "/kaggle/input/chinese-char-recognition-smmo19/test.npy\n",
      "/kaggle/input/chinese-char-recognition-smmo19/train-3.npy\n",
      "/kaggle/input/chinese-char-recognition-smmo19/train-2.npy\n",
      "/kaggle/input/chinese-char-recognition-smmo19/train-4.npy\n",
      "/kaggle/input/chinese-char-recognition-smmo19/train-1.npy\n",
      "/kaggle/input/chinese-char-recognition-smmo19/random_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.ndarray(shape=(0, 2), dtype=np.float32)\n",
    "\n",
    "for i in range(1, 5):\n",
    "    data = np.load(f\"/kaggle/input/chinese-char-recognition-smmo19/train-{i}.npy\", allow_pickle=True)\n",
    "    data_train = np.concatenate([data_train, data])\n",
    "    gc.collect()\n",
    "\n",
    "x_test = np.load(f\"/kaggle/input/chinese-char-recognition-smmo19/test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 143\n",
    "WIDTH = 128\n",
    "batch_size = 64  # 32\n",
    "val_size = 0\n",
    "unique = np.unique(data_train[:, 1])\n",
    "LABELS = len(unique)\n",
    "char_to_id = {}\n",
    "for label in range(LABELS):\n",
    "    char_to_id[unique[label]] = label\n",
    "\n",
    "\n",
    "def train_gen():\n",
    "    for img, label in data_train[int(len(data_train) * val_size):]:\n",
    "        img = img[..., None]\n",
    "        yield img, char_to_id[label]\n",
    "\n",
    "\n",
    "def preprocess_train(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(\n",
    "        x, HEIGHT, WIDTH)\n",
    "    x = x / 255 - 0.5\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(\n",
    "        x, HEIGHT + 14, WIDTH + 13)\n",
    "    x = tf.image.random_crop(x, [HEIGHT, WIDTH, 1])\n",
    "    # x = tf.image.central_crop(x, central_fraction=0.7)\n",
    "    # x = tf.image.random_brightness(x, 0.05)\n",
    "    # x = tf.image.random_contrast(x, 0.7, 1.3)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def test_gen():\n",
    "    for img in x_test:\n",
    "        img = img[..., None]\n",
    "        yield img\n",
    "\n",
    "\n",
    "def preprocess_test(x):\n",
    "    x = tf.image.resize_with_crop_or_pad(x, HEIGHT, WIDTH)\n",
    "    x = x / 255 - 0.5\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперимент покзал, что сильный preprocess для картинок с иероглифами не дает особого прироста качества, но значительно увеличивает время обучения модели. Поэтому был вклчен только самый необходимый препроцессинг.\n",
    "\n",
    "HEIGHT = 143 и WIDTH = 128 были выбраны в соответствии со средним значением размера картинок.\n",
    "\n",
    "Для итогового предсказания был установлен val_size = 0, чтобы обучиться на всех данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_generator(train_gen,\n",
    "                                          output_types=(tf.float32, tf.int32),\n",
    "                                          output_shapes=((None, None, 1), ())\n",
    "                                          ).map(preprocess_train, num_parallel_calls=-1).map(preprocess, num_parallel_calls=-1).prefetch(-1).shuffle(1024).batch(batch_size).repeat()\n",
    "\n",
    "ds_test = tf.data.Dataset.from_generator(test_gen,\n",
    "                                         output_types=(tf.float32),\n",
    "                                         output_shapes=((None, None, 1))\n",
    "                                         ).map(preprocess_test, num_parallel_calls=-1).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    initializer = tf.keras.initializers.glorot_normal()\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(Conv2D(filters=64, padding='same', kernel_size=(3, 3), input_shape=(HEIGHT, WIDTH, 1), kernel_initializer=initializer))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Conv2D(filters=64, padding='same', kernel_size=(3, 3), kernel_initializer=initializer))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters=128, padding='same', kernel_size=(3, 3), kernel_initializer=initializer))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(Conv2D(filters=128, padding='same', kernel_size=(3, 3), kernel_initializer=initializer))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(filters=256, padding='same', kernel_size=(3, 3), kernel_initializer=initializer))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(Conv2D(filters=256, padding='same', kernel_size=(3, 3), kernel_initializer=initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # model.add(Conv2D(filters=256, padding='same', kernel_size=(3,3), kernel_initializer=initializer))\n",
    "    # model.add(LeakyReLU(0.1))\n",
    "    model.add(Conv2D(filters=512, padding='same', kernel_size=(3, 3), kernel_initializer=initializer))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(Conv2D(filters=512, padding='same', kernel_size=(3, 3), kernel_initializer=initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, kernel_initializer=initializer))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(LABELS, kernel_initializer=initializer))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представленная модель состоит из 4-х блоков сверток (примерная схема была уже опробована мной в третьей домашке). Для двух последних блоков сверток применена Батч Нормализация. Для каждого из блоков применен MaxPooling2D. Также происходит уменьшение параметров функций LeakyReLU и Dropout, чтобы не выбросить слишком много важной информации в конце. Последний слой Dense на 1024 дал ключевой прирост в качестве, что помогло продвинуться в лидерборде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 143, 128, 64)      640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 143, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 143, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 143, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 72, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 72, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 72, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 72, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 72, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 36, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 36, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 36, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 36, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 36, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 18, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 18, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 18, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 18, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 18, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 9, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              37749760  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              1025000   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 43,462,056\n",
      "Trainable params: 43,460,520\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 5200 steps\n",
      "Epoch 1/20\n",
      "5200/5200 - 842s - loss: 4.0910 - accuracy: 0.3903\n",
      "Epoch 2/20\n",
      "5200/5200 - 838s - loss: 0.3334 - accuracy: 0.9082\n",
      "Epoch 3/20\n",
      "5200/5200 - 836s - loss: 0.1725 - accuracy: 0.9512\n",
      "Epoch 4/20\n",
      "5200/5200 - 836s - loss: 0.1142 - accuracy: 0.9669\n",
      "Epoch 5/20\n",
      "5200/5200 - 837s - loss: 0.0825 - accuracy: 0.9761\n",
      "Epoch 6/20\n",
      "5200/5200 - 837s - loss: 0.0628 - accuracy: 0.9816\n",
      "Epoch 7/20\n",
      "5200/5200 - 836s - loss: 0.0493 - accuracy: 0.9853\n",
      "Epoch 8/20\n",
      "5200/5200 - 836s - loss: 0.0405 - accuracy: 0.9875\n",
      "Epoch 9/20\n",
      "5200/5200 - 836s - loss: 0.0335 - accuracy: 0.9901\n",
      "Epoch 10/20\n",
      "5200/5200 - 835s - loss: 0.0276 - accuracy: 0.9916\n",
      "Epoch 11/20\n",
      "5200/5200 - 836s - loss: 0.0236 - accuracy: 0.9926\n",
      "Epoch 12/20\n",
      "5200/5200 - 836s - loss: 0.0205 - accuracy: 0.9937\n",
      "Epoch 13/20\n",
      "5200/5200 - 836s - loss: 0.0176 - accuracy: 0.9947\n",
      "Epoch 14/20\n",
      "5200/5200 - 835s - loss: 0.0151 - accuracy: 0.9954\n",
      "Epoch 15/20\n",
      "5200/5200 - 836s - loss: 0.0136 - accuracy: 0.9957\n",
      "Epoch 16/20\n",
      "5200/5200 - 836s - loss: 0.0122 - accuracy: 0.9963\n",
      "Epoch 17/20\n",
      "5200/5200 - 836s - loss: 0.0104 - accuracy: 0.9967\n",
      "Epoch 18/20\n",
      "5200/5200 - 831s - loss: 0.0095 - accuracy: 0.9970\n",
      "Epoch 19/20\n",
      "5200/5200 - 828s - loss: 0.0088 - accuracy: 0.9972\n",
      "Epoch 20/20\n",
      "5200/5200 - 828s - loss: 0.0080 - accuracy: 0.9974\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 5e-3\n",
    "EPOCHS = 20\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adamax(lr=INIT_LR),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return INIT_LR * 0.9 ** epoch\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    steps_per_epoch=5200,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_scheduler),\n",
    "               # ModelSaveCallback(model_filename)\n",
    "               ],\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение steps_per_epoch=5200 выбрано, исходя из размера батча и общего размера выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict_classes(ds_test, batch_size=None)\n",
    "pred = [unique[i] for i in res]\n",
    "df = pd.DataFrame({'Id': range(1, len(pred) + 1), 'Category': pred})\n",
    "df.to_csv('KagglePrediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный код был полностью запущен на Kaggle, выбранные мной предсказания для лидерборда именно оттуда (при желании могу предоставить ссылку)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
